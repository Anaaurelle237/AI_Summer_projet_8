{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../Data/db/fraude_detection_warehouse_.db')\n",
    "\n",
    "alert = pd.read_sql_query(\"SELECT * FROM alerts\", conn)\n",
    "customers = pd.read_sql_query(\"SELECT * FROM customers\", conn)\n",
    "devices = pd.read_sql_query(\"SELECT * FROM devices\", conn)\n",
    "transaction_history = pd.read_sql_query(\"SELECT * FROM transaction_history\", conn)\n",
    "transaction_patterns= pd.read_sql_query(\"SELECT * FROM transaction_patterns\", conn)\n",
    "transactions = pd.read_sql_query(\"SELECT * FROM transactions\", conn)\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des tables\n",
    "df1 = transactions.merge(customers, on='customer_id', how='left').drop(columns=['first_name', 'last_name', 'transaction_date'])\n",
    "df2 = df1.merge(devices, on='device_id', how='left')\n",
    "df3 = df2.merge(transaction_history, on=['customer_id', 'transaction_id'], how='left')\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\"registration_date\", \"date_of_birth\", \"transaction_date\", \"last_used\"]\n",
    "\n",
    "for column in date_columns:\n",
    "    df3[column] = pd.to_datetime(df3[column])\n",
    "\n",
    "df3.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df3.copy()\n",
    "data.drop(columns=[\"transaction_id\", \"customer_id\", \"device_id\", \"email\", \"phone_number\", \"address\", \"history_id\"], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "num_columns = list(data.select_dtypes([\"float64\"]).columns)\n",
    "for col in num_columns:\n",
    "    data[col] = scaler.fit_transform(data[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des variables categorielles\n",
    "le = LabelEncoder()\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    data[col] = le.fit_transform(data[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les composantes de date\n",
    "def extract_date_features(data, column):\n",
    "    data[f'{column}_day'] = data[column].dt.day\n",
    "    data[f'{column}_month'] = data[column].dt.month\n",
    "    data[f'{column}_year'] = data[column].dt.year\n",
    "    data[f'{column}_weekday'] = data[column].dt.weekday\n",
    "    data[f'{column}_quarter'] = data[column].dt.quarter\n",
    "    return data\n",
    "\n",
    "# Conversion des dates et extraction des caractéristiques\n",
    "date_columns = ['transaction_date','last_used', 'registration_date'] # 'date_of_birth'\n",
    "for col in date_columns:\n",
    "    data = extract_date_features(data, col)\n",
    "    \n",
    "    # Suppression de la colonne de date originale\n",
    "    data = data.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner uniquement les colonnes de date nouvellement créées\n",
    "date_columns = [col for col in data.columns if any(x in col for x in ['_day', '_month', '_year', '_weekday', '_quarter'])]\n",
    "\n",
    "# Afficher les premières lignes de ces colonnes\n",
    "print(data[date_columns].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_of_today = datetime.now()\n",
    "\n",
    "def calculate_and_assign(row):\n",
    "  years_since_birth = int((date_of_today - row['date_of_birth']).days / 360)\n",
    "  return pd.Series({'year_since_birth': years_since_birth})\n",
    "\n",
    "data[\"year_since_birth\"] = data.apply(calculate_and_assign, axis=1)\n",
    "data.drop(columns=[\"date_of_birth\"], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lignes_avec_manquantes = df3[df3.isnull().any(axis=1)]\n",
    "\n",
    "nombre_lignes_manquantes = lignes_avec_manquantes.shape[0]\n",
    "\n",
    "print(\"Nombre de lignes avec des valeurs manquantes :\", nombre_lignes_manquantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de correlation\n",
    "corr_matrix = data.corr()\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Matrice de correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection des variables les plus correlees avec is_fraud\n",
    "correlations_with_fraud = corr_matrix['is_fraud'].abs().sort_values(ascending=False)\n",
    "print(\"Top 10 variables les plus corrélées avec is_fraud:\")\n",
    "print(correlations_with_fraud.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous utiliserons les régions comme localisation par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information mutuelle\n",
    "X = data.drop(columns=['is_fraud']) #Contient les variables explicatives (indépendantes)\n",
    "y = data['is_fraud'] #Contient la variable cible (dépendante)\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "print(\"\\nTop 10 variables selon l'information mutuelle:\")\n",
    "print(mi_scores.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection finale des variables importantes\n",
    "important_features = list(set(list(correlations_with_fraud.head(5).sort_values(ascending=False).index) + list(mi_scores.head(5).sort_values(ascending=False).index)))\n",
    "important_features = [f for f in important_features if f != 'is_fraud']\n",
    "print(\"\\nVariables importantes sélectionnées:\")\n",
    "print(len(important_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fusionner les dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation du dataset final\n",
    "X_final = data[important_features]\n",
    "y_final = data['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modele avec Isolation Forest\n",
    "model = IsolationForest(contamination=0.5, random_state=42)\n",
    "y_pred = model.fit_predict(X_final)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluons la precision du modele\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Convertir les prédictions de l'Isolation Forest (-1 pour anomalie, 1 pour normal) \n",
    "# en format binaire (1 pour anomalie, 0 pour normal)\n",
    "#y_pred_binary = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "\n",
    "y_pred_binary = np.where(y_pred==-1, 0, y_pred)\n",
    "\n",
    "# Calculer et afficher les métriques de classification\n",
    "print(classification_report(y_final, y_pred_binary))\n",
    "\n",
    "# Calculer l'AUC-ROC\n",
    "auc_roc = roc_auc_score(y_final, y_pred_binary)\n",
    "print(f\"AUC-ROC: {auc_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_binary[:20])\n",
    "print(y_pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher la matrice de confusion\n",
    "print(\"Matrice de confusion:\")\n",
    "print(confusion_matrix(y_final, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "print(\"\\nNombre d'anomalies détectées:\", sum(y_pred == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=chi2)\n",
    "fit = test.fit(data.drop(columns=\"is_fraud\").abs(), list(y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "features = fit.transform(data.drop(columns=\"is_fraud\"))\n",
    "print(features[0:5, :])\n",
    "feat = pd.DataFrame()\n",
    "feat[\"num_features\"] = data.drop(columns=\"is_fraud\").columns\n",
    "feat[\"score\"] = fit.scores_\n",
    "feat.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "feat = feat.reset_index().drop(columns=[\"index\"])\n",
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_fit_transform(features):\n",
    "    X  = data[features]\n",
    "    model_1 = IsolationForest(contamination=0.5, random_state=42)\n",
    "    model_1.fit(X)\n",
    "\n",
    "    pred_1= model_1.predict(X)\n",
    "    pred_1 = np.where(pred_1==1, 0, pred_1)\n",
    "    pred_1 = np.where(pred_1==-1, 1, pred_1)\n",
    "    print(f\"{pred_1[:10]}\\n\\n\")\n",
    "\n",
    "    #evaluation\n",
    "    print(roc_auc_score(y_final, pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = list(feat.head(4).num_features)\n",
    "print(len(important_features))\n",
    "print(len(feature_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_transform(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_transform(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification des customers par cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"region_e\"] = le.fit_transform(customers.region)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(data.drop(columns=[\"anomalie\", \"transaction_id\", \"customer_id\", \"email\", \"transaction_date\", \"phone_number\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(random_state=42)\n",
    "kmeans.fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyther_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
