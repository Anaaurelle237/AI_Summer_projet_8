{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             \n",
    "import sqlite3                  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.ensemble import RandomForestClassifier   # Importation du classificateur RandomForest\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Importation des fonctions d'évaluation du modèle\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion à la base de données SQLite\n",
    "conn = sqlite3.connect('../Data/db/fraude_detection_warehouse_.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1: Chargement les données de la table transactions\n",
    "data = pd.read_sql(\"SELECT * FROM transactions\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('is_fraud', axis=1)   # Suppression de la colonne is_fraud' pour obtenir les caractéristiques\n",
    "y = data['is_fraud'] # La cible est la colonne 'is_fraud'\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la colonne 'transaction_date' en type datetime\n",
    "X['transaction_date'] = pd.to_datetime(X['transaction_date'], errors='coerce')\n",
    "\n",
    "# Extraire jour, mois, et année de la colonne 'transaction_date'\n",
    "X['jour'] = X['transaction_date'].dt.day\n",
    "X['mois'] = X['transaction_date'].dt.month\n",
    "X['annee'] = X['transaction_date'].dt.year\n",
    "\n",
    "# Supprimer la colonne 'transaction_date'\n",
    "X = X.drop(columns=['transaction_date'])\n",
    "\n",
    "# Vérifier les changements\n",
    "print(X.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les colonnes ID qui ne sont pas des features pertinentes\n",
    "X = X.drop(columns=['transaction_id', 'customer_id', 'device_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Liste des colonnes catégorielles à encoder\n",
    "cols_to_encode = ['transaction_type', 'location', 'status', ]\n",
    "\n",
    "# Encoder les colonnes en valeurs numériques\n",
    "label_encoder = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    X[col] = label_encoder.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Vérifier les changements\n",
    "print(X.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3: Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# `test_size=0.2` signifie que 20% des données seront utilisées pour le test, 80% pour l'entraînement\n",
    "# `random_state=42` est un paramètre pour reproduire les résultats aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle RandomForest\n",
    "modele = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "modele.fit(X_train, y_train)  # Entraînement du modèle sur les données d’entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modele.predict(X_test)  # Prédiction des données de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "# Calculer la précision\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Précision (Accuracy) : {accuracy:.4f}\")\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix)\n",
    "\n",
    "# Afficher un rapport de classification détaillé (précision, rappel, F1-score)\n",
    "print(\"Rapport de classification :\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculer le score F1\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Score F1 : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETATION: notre modele a  62,63 % de predictions correctes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LE BAGGING #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION DE NOTRE MODELE DE BAGGING\n",
    "\n",
    "# Définissons le modèle de base (RandomForestClassifier)\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=15)\n",
    "\n",
    "# Création du BaggingClassifier avec notre modèle de base\n",
    "bagging_model = BaggingClassifier(estimator=base_model, \n",
    "                                  n_estimators=10,   # Nombre de modèles à entraîner\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1)  # Utilisation de tous les cœurs CPU disponibles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînons le modèle  sur nos données d'entraînement\n",
    "bagging_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  prédictions sur les données de test\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Calcul de la précision (Accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Précision (Accuracy) : {accuracy:.4f}\")\n",
    "\n",
    "#  matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# rapport de classification\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Rapport de classification :\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETATION: notre modele a  65,38 % de predictions correctes, soit 2.75% de plus que notre modele de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LE BOOSTING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECHERCHE DES MEILLEURS HYPERPARAMETRES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Définir la distribution des paramètres à tester\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'colsample_bytree': uniform(0.7, 0.3),\n",
    "    'gamma': uniform(0, 0.5)\n",
    "}\n",
    "\n",
    "# Créer le modèle XGBoost\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "\n",
    "# Configurer RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist, n_iter=50, cv=3, scoring='f1', verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Exécuter la recherche aléatoire\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "print(f\"Meilleurs paramètres trouvés : {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer un modèle XGBoost avec les meilleurs hyperparamètres trouvés\n",
    "\n",
    "best_xgb_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.7816396748153905,\n",
    "    gamma=0.32384506027068116,\n",
    "    learning_rate=0.010156113098594747,\n",
    "    max_depth=7,\n",
    "    n_estimators=332,\n",
    "    subsample=0.7914343774474086,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour les données de test\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Précision (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"Score F1: {f1:.4f}\")\n",
    "print(\"Matrice de confusion :\\n\", confusion)\n",
    "print(\"Rapport de classification :\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# la precision de notre modele de boosting  est de 66,29%, soit 1% plus precis que le bagging. \n",
    "\n",
    "### Cherchons a comprendre si c'est la technique qui est meilleure pour notre cas ou alors si c'est grace à l'utilisation des meilleurrs hyperparametres que nous avons de meilleurs resultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING AVEC LES HYPERPARAMETRES TROUVES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Créer le classifieur de base avec les hyperparamètres optimisés\n",
    "base_classifier = DecisionTreeClassifier(\n",
    "    max_depth=7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Créer le modèle Bagging avec les paramètres optimisés\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=base_classifier,\n",
    "    n_estimators=332,  # équivalent à n_estimators dans XGBoost\n",
    "    max_samples=0.7914343774474086,  # équivalent à subsample dans XGBoost\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour les données de test\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "f1_bagging = f1_score(y_test, y_pred_bagging)\n",
    "confusion_bagging = confusion_matrix(y_test, y_pred_bagging)\n",
    "classification_rep_bagging = classification_report(y_test, y_pred_bagging)\n",
    "\n",
    "print(f\"Précision (Accuracy): {accuracy_bagging:.4f}\")\n",
    "print(f\"Score F1: {f1_bagging:.4f}\")\n",
    "print(\"Matrice de confusion :\\n\", confusion_bagging)\n",
    "print(\"Rapport de classification :\\n\", classification_rep_bagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETATION #\n",
    "\n",
    "## PREDICTIONS CORRECTES: 66,36%\n",
    "\n",
    "Vrais négatifs (TN) : 5661 / non fraudes classées comme non fraudes\n",
    "Faux positifs (FP) : 5148  / non fraudes classées comme fraude\n",
    "Faux négatifs (FN) : 3263 / fraudes classés comme non fraudes\n",
    "Vrais positifs (TP) : 10 928 / fraudes classées comme fraude\n",
    "## FRAUDES DETECTEES: 10 928 et FRAUDES NON DETECTEES: 3263\n",
    "\n",
    "63% de prediction correctes sur la classe de non fraude et 68% de predictions correctes sur la classe des fraudes \n",
    "## (le f1 score nous montre que notre modele a mieux detecter les cas de fraudes que de non fraude)\n",
    "\n",
    "# conclusion: \n",
    "\n",
    "### l'utilisaton des meilleurs hyperparametres couplé avec un arbre de decision comme modele de base a amelioré la precision de notre modele de Bagging de 1% par rapport au modele de bagging utilisant des hyperparametres aleatoires .  \n",
    "\n",
    "### En comparant les performances de nos methodes (bagging et boosting) couplés avec l'utilisation des hyperparametres trouvés, on se rend compte que les resultats en utilisant le bagging sont pratiquement egales(0.07% de difference). \n",
    "\n",
    "# On conclut que utiliser un modèle d'ensemble a amélioré nos resultats. Cependant, le choix parmi les deux modèles d'ensemble a eu peu d'influence sur nos resultats mais utiliser les meilleurs hyperparametres a ete le facteur ayant le plus amelioré les resultats de nos 2 modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Entraînement du modèle (par exemple, un modèle RandomForest)\n",
    "# modèle = RandomForestClassifier(...)\n",
    "# modèle.fit(X_train, y_train)\n",
    "\n",
    "# Enregistrer le modèle entraîné\n",
    "joblib.dump(bagging_model, 'bagging_model.pkl')\n",
    "\n",
    "print(\"Le modèle a été enregistré avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle entraîné\n",
    "# bagging_model_reloaded = joblib.load('bagging_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
